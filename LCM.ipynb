{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39f46258",
   "metadata": {},
   "source": [
    "## LCM Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee1edb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import constraint\n",
    "from techniques.PaDMTP import padmtp_algo\n",
    "from techniques.RandomTesting import random_testing\n",
    "from testcodes.lcm import compute_lcm as lcm\n",
    "from techniques.AdaptiveRandomTesting import adaptive_random_testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8965a246",
   "metadata": {},
   "source": [
    "### PaDMTP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c29f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = constraint.Problem()\n",
    "\n",
    "problem.addVariable('x', range(1, 16))\n",
    "problem.addVariable('y', range(1, 16))\n",
    "\n",
    "problem.addConstraint(lcm, ['x', 'y'])\n",
    "\n",
    "solutions = problem.getSolutions()\n",
    "\n",
    "length = len(solutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807b29b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning Dataset where values of 'x' or 'y' are 0(zero)\n",
    "\n",
    "cleaned_solutions = []\n",
    "\n",
    "for index, solution in enumerate(solutions):\n",
    "    if solution['x'] == 0 or solution['y'] == 0:\n",
    "        continue\n",
    "    else:\n",
    "        cleaned_solutions.append(solutions[index])\n",
    "        \n",
    "lcm_length = len(cleaned_solutions)\n",
    "\n",
    "print(\"\\nPossible Solutions Dataset for LCM Code: \\n(x,y) âˆˆ {\", end=\"\")\n",
    "for index, solution in enumerate(cleaned_solutions):\n",
    "    if index == lcm_length - 1:\n",
    "        print(\"({},{})\".format(solution['x'], solution['y']), end=\"\")\n",
    "    else:\n",
    "        print(\"({},{}),\".format(solution['x'], solution['y']), end=\"\")\n",
    "print(\"}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206f2c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"LCM - PaDMTP - Test Cases: {lcm_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ce0057",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mutPyObj = 'mut.py --target testcodes.lcm --unit-test mutations.test_lcm_padmt -m'\n",
    "\n",
    "padmtp_rslt = padmtp_algo(lcm, cleaned_solutions, mutPyObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a2a7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "padmtp_rslt['PaDMTP_Overhead']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bb9cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "padmtp_rslt['PaDMTP_DataFrame']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c25aadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "PaDMTP_MT_Scores = []\n",
    "\n",
    "for index, row in padmtp_rslt['PaDMTP_DataFrame'].iterrows():\n",
    "    val = row['MT Score']\n",
    "    val = val.replace(\"%\", \"\")\n",
    "    val = float(val) / 100\n",
    "    val = round(val, 3)\n",
    "    \n",
    "    PaDMTP_MT_Scores.append(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbec0b7c",
   "metadata": {},
   "source": [
    "### Random Testing (RT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417551ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating random inputs\n",
    "rt_test_cases = []\n",
    "for i in range(lcm_length):\n",
    "    x = random.randint(1, 101)\n",
    "    y = random.randint(1, 101)\n",
    "    \n",
    "    rt_test_cases.append((x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bf197f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"LCM - Random Testing - Test Cases: {len(rt_test_cases)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe6bd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving RT Test Cases to JSON File\n",
    "\n",
    "jsonString = json.dumps(rt_test_cases)\n",
    "jsonFile = open(\"./temp/RT_Test_Cases.json\", 'w')\n",
    "jsonFile.write(jsonString)\n",
    "jsonFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d1636f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pytest_cmd = 'pytest techniques/rt_lcm.py'\n",
    "mutpy_cmd = 'mut.py --target testcodes.lcm --unit-test mutations.test_lcm_rt -m'\n",
    "\n",
    "rt_rslt = random_testing(pytest_cmd, mutpy_cmd, rt_test_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f5ce10",
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_rslt['RT_Overhead']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a124dc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_rslt['RT_DataFrame']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec19bcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "RT_MT_Scores = []\n",
    "\n",
    "for index, row in rt_rslt['RT_DataFrame'].iterrows():\n",
    "    val = row['MT Score']\n",
    "    val = val.replace(\"%\", \"\")\n",
    "    val = float(val) / 100\n",
    "    val = round(val, 3)\n",
    "    \n",
    "    RT_MT_Scores.append(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6542878",
   "metadata": {},
   "source": [
    "### Adaptive Random Testing (ART)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffd3f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating random inputs\n",
    "art_test_cases = []\n",
    "for i in range(lcm_length):\n",
    "    x = random.randint(1, 101)\n",
    "    y = random.randint(1, 101)\n",
    "    \n",
    "    art_test_cases.append([x, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119674d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"LCM - Adaptive Random Testing - Test Cases: {len(art_test_cases)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4608d9f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mutpy_cmd = 'mut.py --target testcodes.lcm --unit-test mutations.test_lcm_art -m'\n",
    "\n",
    "art_rslt = adaptive_random_testing(lcm, mutpy_cmd, art_test_cases, 'LCM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfb964a",
   "metadata": {},
   "outputs": [],
   "source": [
    "art_rslt['ART_Overhead']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238ce5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "art_rslt['ART_DataFrame']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9ab048",
   "metadata": {},
   "outputs": [],
   "source": [
    "ART_MT_Scores = []\n",
    "\n",
    "for index, row in art_rslt['ART_DataFrame'].iterrows():\n",
    "    val = row['MT Score']\n",
    "    val = val.replace(\"%\", \"\")\n",
    "    val = float(val) / 100\n",
    "    val = round(val, 3)\n",
    "    \n",
    "    ART_MT_Scores.append(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f24c0d0",
   "metadata": {},
   "source": [
    "### Pairwise T-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adc2f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Define the data\n",
    "PaDMTP = np.array(PaDMTP_MT_Scores) \n",
    "RT = np.array(RT_MT_Scores)\n",
    "ART = np.array(ART_MT_Scores)\n",
    "\n",
    "# Calculate the mean\n",
    "mean_padmtp = np.mean(PaDMTP)\n",
    "mean_rt = np.mean(RT)\n",
    "mean_art = np.mean(ART)\n",
    "\n",
    "# Calculate the variance\n",
    "var_padmtp = np.var(PaDMTP, ddof=1)\n",
    "var_rt = np.var(RT, ddof=1)\n",
    "var_art = np.var(ART, ddof=1)\n",
    "\n",
    "# Get the number of observations\n",
    "n_padmtp = len(PaDMTP)\n",
    "n_rt = len(RT)\n",
    "n_art = len(ART)\n",
    "\n",
    "# Calculate the degrees of freedom\n",
    "df_padmtp = n_padmtp - 1\n",
    "df_rt = n_rt - 1\n",
    "df_art = n_art - 1\n",
    "\n",
    "# Perform t-tests\n",
    "t_stat_rt, _ = stats.ttest_1samp(RT, popmean=mean_padmtp)\n",
    "t_stat_art, _ = stats.ttest_1samp(ART, popmean=mean_padmtp)\n",
    "\n",
    "# Create tables\n",
    "table1 = [\n",
    "    [\"Test\", \"Mean\", \"Variance\", \"Observations\", \"df\", \"t Stat\"],\n",
    "    [\"PaDMTP vs RT\", mean_rt, var_rt, n_rt, df_rt, t_stat_rt],\n",
    "    [\"PaDMTP vs ART\", mean_art, var_art, n_art, df_art, t_stat_art]\n",
    "]\n",
    "\n",
    "print(\"<==== LCM Code ====>\")\n",
    "\n",
    "# Print tables\n",
    "print(\"Table 1:\")\n",
    "print(tabulate(table1, headers=\"firstrow\", tablefmt=\"grid\"))\n",
    "# Perform one-sample t-tests\n",
    "t_stat_rt, p_value_rt = stats.ttest_1samp(RT, popmean=np.mean(PaDMTP))\n",
    "t_stat_art, p_value_art = stats.ttest_1samp(ART, popmean=np.mean(PaDMTP))\n",
    "\n",
    "# Set the significance level (alpha)\n",
    "alpha = 0.05\n",
    "\n",
    "# Calculate the one-tail p-values and critical values\n",
    "p_one_tail_rt = stats.t.cdf(t_stat_rt, df=len(RT)-1)\n",
    "p_one_tail_art = stats.t.cdf(t_stat_art, df=len(ART)-1)\n",
    "t_crit_one_tail_rt = stats.t.ppf(1-alpha, df=len(RT)-1)\n",
    "t_crit_one_tail_art = stats.t.ppf(1-alpha, df=len(ART)-1)\n",
    "\n",
    "# Calculate the two-tail p-values and critical values\n",
    "p_two_tail_rt = 2 * (1 - stats.t.cdf(abs(t_stat_rt), df=len(RT)-1))\n",
    "p_two_tail_art = 2 * (1 - stats.t.cdf(abs(t_stat_art), df=len(ART)-1))\n",
    "t_crit_two_tail_rt = stats.t.ppf(1-alpha/2, df=len(RT)-1)\n",
    "t_crit_two_tail_art = stats.t.ppf(1-alpha/2, df=len(ART)-1)\n",
    "print(\"\\n\\n\")\n",
    "# Create table\n",
    "table2 = [\n",
    "    [\"Test\", \"P(T<=t) one-tail\", \"t Critical one-tail\", \"P(T<=t) two-tail\", \"t Critical two-tail\"],\n",
    "    [\"PaDMTP vs RT\", p_one_tail_rt, t_crit_one_tail_rt, p_two_tail_rt, t_crit_two_tail_rt],\n",
    "    [\"PaDMTP vs ART\", p_one_tail_art, t_crit_one_tail_art, p_two_tail_art, t_crit_two_tail_art]\n",
    "]\n",
    "\n",
    "# Print table\n",
    "print(\"Table 2:\")\n",
    "print(tabulate(table2, headers=\"firstrow\", tablefmt=\"grid\"))\n",
    "print(\"\\n\\n\")\n",
    "# Perform one-sample t-tests\n",
    "t_stat_rt, p_value_rt = stats.ttest_1samp(RT, popmean=0.727)\n",
    "t_stat_art, p_value_art = stats.ttest_1samp(ART, popmean=0.727)\n",
    "print(\"Table 3:\")\n",
    "# Print the results\n",
    "print(\"RT vs PaDMTP:\")\n",
    "print(\"t Statistic:\", t_stat_rt)\n",
    "print(\"p-value:\", p_value_rt)\n",
    "print(\"\\nART vs PaDMTP:\")\n",
    "print(\"t Statistic:\", t_stat_art)\n",
    "print(\"p-value:\", p_value_art)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1176dbc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
